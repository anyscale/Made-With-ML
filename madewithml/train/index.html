<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><link rel="canonical" href="https://madewithml.com/madewithml/train/" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>train - Made With ML</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css" />
        <link href="../../assets/_mkdocstrings.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "train";
        var mkdocs_page_input_path = "madewithml/train.md";
        var mkdocs_page_url = "/madewithml/train/";
      </script>
    
    <script src="../../js/jquery-3.6.0.min.js" defer></script>
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
      <script>hljs.initHighlightingOnLoad();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> Made With ML
        </a>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../..">Home</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">madewithml</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../data/">data</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../models/">models</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="./">train</a>
    <ul class="current">
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tune/">tune</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../evaluate/">evaluate</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../predict/">predict</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../serve/">serve</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../utils/">utils</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">Made With ML</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" alt="Docs"></a> &raquo;</li>
          <li>madewithml &raquo;</li>
      <li>train</li>
    <li class="wy-breadcrumbs-aside">
          <a href="https://github.com/GokuMohandas/Made-With-ML/edit/master/docs/madewithml/train.md" class="icon icon-github"> Edit on GitHub</a>
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <div class="doc doc-object doc-module">


<a id="madewithml.train"></a>
  <div class="doc doc-contents first">

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h2 id="madewithml.train.eval_step" class="doc doc-heading">
<code class="highlight language-python">eval_step(ds, batch_size, model, num_classes, loss_fn)</code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Eval step.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b>ds</b>
                  (<code><span title="ray.data.Dataset">Dataset</span></code>)
              –
              <div class="doc-md-description">
                <p>dataset to iterate batches from.</p>
              </div>
            </li>
            <li>
              <b>batch_size</b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>size of each batch.</p>
              </div>
            </li>
            <li>
              <b>model</b>
                  (<code><span title="torch">nn</span>.<span title="torch.Module">Module</span></code>)
              –
              <div class="doc-md-description">
                <p>model to train.</p>
              </div>
            </li>
            <li>
              <b>num_classes</b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>number of classes.</p>
              </div>
            </li>
            <li>
              <b>loss_fn</b>
                  (<code>torch.<span title="torch.nn">nn</span>.<span title="torch.nn.loss">loss</span>.<span title="torch.nn.loss._WeightedLoss">_WeightedLoss</span></code>)
              –
              <div class="doc-md-description">
                <p>loss function to use between labels and predictions.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
                <code><span title="typing.Tuple">Tuple</span>[float, <span title="numpy">np</span>.<span title="numpy.array">array</span>, <span title="numpy">np</span>.<span title="numpy.array">array</span>]</code>
            –
            <div class="doc-md-description">
              <p>Tuple[float, np.array, np.array]: cumulative loss, ground truths and predictions.</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>madewithml/train.py</code></summary>
            <pre class="highlight"><code class="language-python">def eval_step(
    ds: Dataset, batch_size: int, model: nn.Module, num_classes: int, loss_fn: torch.nn.modules.loss._WeightedLoss
) -&gt; Tuple[float, np.array, np.array]:  # pragma: no cover, tested via train workload
    """Eval step.

    Args:
        ds (Dataset): dataset to iterate batches from.
        batch_size (int): size of each batch.
        model (nn.Module): model to train.
        num_classes (int): number of classes.
        loss_fn (torch.nn.loss._WeightedLoss): loss function to use between labels and predictions.

    Returns:
        Tuple[float, np.array, np.array]: cumulative loss, ground truths and predictions.
    """
    model.eval()
    loss = 0.0
    y_trues, y_preds = [], []
    ds_generator = ds.iter_torch_batches(batch_size=batch_size, collate_fn=utils.collate_fn)
    with torch.inference_mode():
        for i, batch in enumerate(ds_generator):
            z = model(batch)
            targets = F.one_hot(batch["targets"], num_classes=num_classes).float()  # one-hot (for loss_fn)
            J = loss_fn(z, targets).item()
            loss += (J - loss) / (i + 1)
            y_trues.extend(batch["targets"].cpu().numpy())
            y_preds.extend(torch.argmax(z, dim=1).cpu().numpy())
    return loss, np.vstack(y_trues), np.vstack(y_preds)</code></pre>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h2 id="madewithml.train.train_loop_per_worker" class="doc doc-heading">
<code class="highlight language-python">train_loop_per_worker(config)</code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Training loop that each worker will execute.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b>config</b>
                  (<code>dict</code>)
              –
              <div class="doc-md-description">
                <p>arguments to use for training.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>madewithml/train.py</code></summary>
            <pre class="highlight"><code class="language-python">def train_loop_per_worker(config: dict) -&gt; None:  # pragma: no cover, tested via train workload
    """Training loop that each worker will execute.

    Args:
        config (dict): arguments to use for training.
    """
    # Hyperparameters
    dropout_p = config["dropout_p"]
    lr = config["lr"]
    lr_factor = config["lr_factor"]
    lr_patience = config["lr_patience"]
    batch_size = config["batch_size"]
    num_epochs = config["num_epochs"]
    num_classes = config["num_classes"]

    # Get datasets
    utils.set_seeds()
    train_ds = session.get_dataset_shard("train")
    val_ds = session.get_dataset_shard("val")

    # Model
    llm = BertModel.from_pretrained("allenai/scibert_scivocab_uncased", return_dict=False)
    model = models.FinetunedLLM(llm=llm, dropout_p=dropout_p, embedding_dim=llm.config.hidden_size, num_classes=num_classes)
    model = train.torch.prepare_model(model)

    # Training components
    loss_fn = nn.BCEWithLogitsLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode="min", factor=lr_factor, patience=lr_patience)

    # Training
    batch_size_per_worker = batch_size // session.get_world_size()
    for epoch in range(num_epochs):
        # Step
        train_loss = train_step(train_ds, batch_size_per_worker, model, num_classes, loss_fn, optimizer)
        val_loss, _, _ = eval_step(val_ds, batch_size_per_worker, model, num_classes, loss_fn)
        scheduler.step(val_loss)

        # Checkpoint
        metrics = dict(epoch=epoch, lr=optimizer.param_groups[0]["lr"], train_loss=train_loss, val_loss=val_loss)
        checkpoint = TorchCheckpoint.from_model(model=model)
        session.report(metrics, checkpoint=checkpoint)</code></pre>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h2 id="madewithml.train.train_model" class="doc doc-heading">
<code class="highlight language-python">train_model(experiment_name=None, dataset_loc=None, train_loop_config=None, num_workers=1, cpu_per_worker=1, gpu_per_worker=0, num_samples=None, num_epochs=1, batch_size=256, results_fp=None)</code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Main train function to train our model as a distributed workload.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b>experiment_name</b>
                  (<code>str</code>)
              –
              <div class="doc-md-description">
                <p>name of the experiment for this training workload.</p>
              </div>
            </li>
            <li>
              <b>dataset_loc</b>
                  (<code>str</code>)
              –
              <div class="doc-md-description">
                <p>location of the dataset.</p>
              </div>
            </li>
            <li>
              <b>train_loop_config</b>
                  (<code>str</code>)
              –
              <div class="doc-md-description">
                <p>arguments to use for training.</p>
              </div>
            </li>
            <li>
              <b>num_workers</b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>number of workers to use for training. Defaults to 1.</p>
              </div>
            </li>
            <li>
              <b>cpu_per_worker</b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>number of CPUs to use per worker. Defaults to 1.</p>
              </div>
            </li>
            <li>
              <b>gpu_per_worker</b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>number of GPUs to use per worker. Defaults to 0.</p>
              </div>
            </li>
            <li>
              <b>num_samples</b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>number of samples to use from dataset.
If this is passed in, it will override the config. Defaults to None.</p>
              </div>
            </li>
            <li>
              <b>num_epochs</b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>number of epochs to train for.
If this is passed in, it will override the config. Defaults to None.</p>
              </div>
            </li>
            <li>
              <b>batch_size</b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>number of samples per batch.
If this is passed in, it will override the config. Defaults to None.</p>
              </div>
            </li>
            <li>
              <b>results_fp</b>
                  (<code>str</code>)
              –
              <div class="doc-md-description">
                <p>filepath to save results to. Defaults to None.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
                <code>ray.<span title="ray.air">air</span>.<span title="ray.air.result">result</span>.<span title="ray.air.result.Result">Result</span></code>
            –
            <div class="doc-md-description">
              <p>ray.air.result.Result: training results.</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>madewithml/train.py</code></summary>
            <pre class="highlight"><code class="language-python">@app.command()
def train_model(
    experiment_name: Annotated[str, typer.Option(help="name of the experiment for this training workload.")] = None,
    dataset_loc: Annotated[str, typer.Option(help="location of the dataset.")] = None,
    train_loop_config: Annotated[str, typer.Option(help="arguments to use for training.")] = None,
    num_workers: Annotated[int, typer.Option(help="number of workers to use for training.")] = 1,
    cpu_per_worker: Annotated[int, typer.Option(help="number of CPUs to use per worker.")] = 1,
    gpu_per_worker: Annotated[int, typer.Option(help="number of GPUs to use per worker.")] = 0,
    num_samples: Annotated[int, typer.Option(help="number of samples to use from dataset.")] = None,
    num_epochs: Annotated[int, typer.Option(help="number of epochs to train for.")] = 1,
    batch_size: Annotated[int, typer.Option(help="number of samples per batch.")] = 256,
    results_fp: Annotated[str, typer.Option(help="filepath to save results to.")] = None,
) -&gt; ray.air.result.Result:
    """Main train function to train our model as a distributed workload.

    Args:
        experiment_name (str): name of the experiment for this training workload.
        dataset_loc (str): location of the dataset.
        train_loop_config (str): arguments to use for training.
        num_workers (int, optional): number of workers to use for training. Defaults to 1.
        cpu_per_worker (int, optional): number of CPUs to use per worker. Defaults to 1.
        gpu_per_worker (int, optional): number of GPUs to use per worker. Defaults to 0.
        num_samples (int, optional): number of samples to use from dataset.
            If this is passed in, it will override the config. Defaults to None.
        num_epochs (int, optional): number of epochs to train for.
            If this is passed in, it will override the config. Defaults to None.
        batch_size (int, optional): number of samples per batch.
            If this is passed in, it will override the config. Defaults to None.
        results_fp (str, optional): filepath to save results to. Defaults to None.

    Returns:
        ray.air.result.Result: training results.
    """
    # Set up
    train_loop_config = json.loads(train_loop_config)
    train_loop_config["num_samples"] = num_samples
    train_loop_config["num_epochs"] = num_epochs
    train_loop_config["batch_size"] = batch_size

    # Scaling config
    scaling_config = ScalingConfig(
        num_workers=num_workers,
        use_gpu=bool(gpu_per_worker),
        resources_per_worker={"CPU": cpu_per_worker, "GPU": gpu_per_worker},
        _max_cpu_fraction_per_node=0.8,
    )

    # Checkpoint config
    checkpoint_config = CheckpointConfig(
        num_to_keep=1,
        checkpoint_score_attribute="val_loss",
        checkpoint_score_order="min",
    )

    # MLflow callback
    mlflow_callback = MLflowLoggerCallback(
        tracking_uri=MLFLOW_TRACKING_URI,
        experiment_name=experiment_name,
        save_artifact=True,
    )

    # Run config
    run_config = RunConfig(
        callbacks=[mlflow_callback],
        checkpoint_config=checkpoint_config,
    )

    # Dataset
    ds = data.load_data(dataset_loc=dataset_loc, num_samples=train_loop_config["num_samples"])
    train_ds, val_ds = data.stratify_split(ds, stratify="tag", test_size=0.2)
    tags = train_ds.unique(column="tag")
    train_loop_config["num_classes"] = len(tags)

    # Dataset config
    dataset_config = {
        "train": DatasetConfig(fit=False, transform=False, randomize_block_order=False),
        "val": DatasetConfig(fit=False, transform=False, randomize_block_order=False),
    }

    # Preprocess
    preprocessor = data.CustomPreprocessor()
    train_ds = preprocessor.fit_transform(train_ds)
    val_ds = preprocessor.transform(val_ds)
    train_ds = train_ds.materialize()
    val_ds = val_ds.materialize()

    # Trainer
    trainer = TorchTrainer(
        train_loop_per_worker=train_loop_per_worker,
        train_loop_config=train_loop_config,
        scaling_config=scaling_config,
        run_config=run_config,
        datasets={"train": train_ds, "val": val_ds},
        dataset_config=dataset_config,
        preprocessor=preprocessor,
    )

    # Train
    results = trainer.fit()
    d = {
        "timestamp": datetime.datetime.now().strftime("%B %d, %Y %I:%M:%S %p"),
        "run_id": utils.get_run_id(experiment_name=experiment_name, trial_id=results.metrics["trial_id"]),
        "params": results.config["train_loop_config"],
        "metrics": utils.dict_to_list(results.metrics_dataframe.to_dict(), keys=["epoch", "train_loss", "val_loss"]),
    }
    logger.info(json.dumps(d, indent=2))
    if results_fp:  # pragma: no cover, saving results
        utils.save_dict(d, results_fp)
    return results</code></pre>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h2 id="madewithml.train.train_step" class="doc doc-heading">
<code class="highlight language-python">train_step(ds, batch_size, model, num_classes, loss_fn, optimizer)</code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Train step.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b>ds</b>
                  (<code><span title="ray.data.Dataset">Dataset</span></code>)
              –
              <div class="doc-md-description">
                <p>dataset to iterate batches from.</p>
              </div>
            </li>
            <li>
              <b>batch_size</b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>size of each batch.</p>
              </div>
            </li>
            <li>
              <b>model</b>
                  (<code><span title="torch">nn</span>.<span title="torch.Module">Module</span></code>)
              –
              <div class="doc-md-description">
                <p>model to train.</p>
              </div>
            </li>
            <li>
              <b>num_classes</b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>number of classes.</p>
              </div>
            </li>
            <li>
              <b>loss_fn</b>
                  (<code>torch.<span title="torch.nn">nn</span>.<span title="torch.nn.loss">loss</span>.<span title="torch.nn.loss._WeightedLoss">_WeightedLoss</span></code>)
              –
              <div class="doc-md-description">
                <p>loss function to use between labels and predictions.</p>
              </div>
            </li>
            <li>
              <b>optimizer</b>
                  (<code>torch.<span title="torch.optimizer">optimizer</span>.<span title="torch.optimizer.Optimizer">Optimizer</span></code>)
              –
              <div class="doc-md-description">
                <p>optimizer to use for updating the model's weights.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
<b>float</b>(                <code>float</code>
)            –
            <div class="doc-md-description">
              <p>cumulative loss for the dataset.</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary> <code>madewithml/train.py</code></summary>
            <pre class="highlight"><code class="language-python">def train_step(
    ds: Dataset,
    batch_size: int,
    model: nn.Module,
    num_classes: int,
    loss_fn: torch.nn.modules.loss._WeightedLoss,
    optimizer: torch.optim.Optimizer,
) -&gt; float:  # pragma: no cover, tested via train workload
    """Train step.

    Args:
        ds (Dataset): dataset to iterate batches from.
        batch_size (int): size of each batch.
        model (nn.Module): model to train.
        num_classes (int): number of classes.
        loss_fn (torch.nn.loss._WeightedLoss): loss function to use between labels and predictions.
        optimizer (torch.optimizer.Optimizer): optimizer to use for updating the model's weights.

    Returns:
        float: cumulative loss for the dataset.
    """
    model.train()
    loss = 0.0
    ds_generator = ds.iter_torch_batches(batch_size=batch_size, collate_fn=utils.collate_fn)
    for i, batch in enumerate(ds_generator):
        optimizer.zero_grad()  # reset gradients
        z = model(batch)  # forward pass
        targets = F.one_hot(batch["targets"], num_classes=num_classes).float()  # one-hot (for loss_fn)
        J = loss_fn(z, targets)  # define loss
        J.backward()  # backward pass
        optimizer.step()  # update weights
        loss += (J.detach().item() - loss) / (i + 1)  # cumulative loss
    return loss</code></pre>
          </details>
  </div>

</div>



  </div>

  </div>

</div>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../models/" class="btn btn-neutral float-left" title="models"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../tune/" class="btn btn-neutral float-right" title="tune">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
        <span>
          <a href="https://github.com/GokuMohandas/Made-With-ML/" class="fa fa-github" style="color: #fcfcfc"> GitHub</a>
        </span>
    
    
      <span><a href="../models/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../tune/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme_extra.js" defer></script>
    <script src="../../js/theme.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
